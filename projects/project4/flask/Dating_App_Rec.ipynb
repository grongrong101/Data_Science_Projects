{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "import pandas as pd\n",
    "pbar = ProgressBar()\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from langdetect import detect \n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = flask.Flask(__name__)  # create instance of Flask class\n",
    "\n",
    "with open(\"doc_topic_nmf.pkl\", \"rb\") as f:\n",
    "    doc_topic_nmf = pickle.load(f)\n",
    "    \n",
    "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open(\"nmf_model.pkl\", \"rb\") as f:\n",
    "    nmf_model = pickle.load(f)\n",
    "    \n",
    "with open(\"reviews.pkl\", \"rb\") as f:\n",
    "    reviews = pickle.load(f)\n",
    "    \n",
    "with open(\"doc_word.pkl\", \"rb\") as f:\n",
    "    doc_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal example from:\n",
    "# http://flask.pocoo.org/docs/quickstart/\n",
    "    \n",
    "def lang(sentence):\n",
    "    try:\n",
    "        return detect(sentence)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    manual_stop = ['bumble', 'tinder', 'zoosks', 'zoosk', 'bumble', 'east meets east', 'grindr', 'hinge',\n",
    "                  'cmb', 'okcupid','bumble', 'tinders', 'zoosks', 'zoosk', 'bumbles', 'east meets east', 'grindrs', 'hinges',\n",
    "                  'cmbs', 'okcupids']\n",
    "    stop.extend(manual_stop)\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "def clean_again_text(text):\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    manual_stop = ['bumble', 'tinder', 'zoosks', 'zoosk', 'bumble', 'east meets east', 'grindr', 'hinge','okc',\n",
    "                  'cmb', 'okcupid','bumble', 'tinders', 'zoosks', 'zoosk', 'bumbles', 'east meets east', 'grindrs', 'hinges',\n",
    "                  'cmbs', 'okcupids', 'im', 'i','think','ive', 'want', 'lol', 'haha' , 'cupid', 'ok', 'let', 'know',\n",
    "                  'right','dont','lot','happn','league','blk','jswipe']\n",
    "    stop.extend(manual_stop)\n",
    "    stop.extend(STOP_WORDS)\n",
    "    \n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "def translate_name(app):\n",
    "    return app_name_dict[app]\n",
    "    \n",
    "def get_related_app(phrase):\n",
    "    phrase = clean_text(phrase)\n",
    "    phrase = re.sub('([^\\x00-\\x7F])+', '', phrase)\n",
    "    phrase = re.sub(r\"[\\/\\-\\'\\)\\(,.;@#?!&$]+\\ *\", \" \", phrase)\n",
    "    phrase = clean_again_text(phrase)\n",
    "    phrase = [phrase]\n",
    "    phrase_doc_word = tfidf_vectorizer.transform(phrase)\n",
    "    phrase_doc_topic = nmf_model.transform(phrase_doc_word)\n",
    "    df_doc = doc_topic_nmf\n",
    "    d = pairwise_distances(doc_word,phrase_doc_word,metric='cosine')\n",
    "    #[films[m] for m in d[1].argsort()[:10]]\n",
    "    df = pd.DataFrame(data=d, columns=[\"signal\"])\n",
    "    df_merged = df.merge(df_doc, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    df_fin_merged = df_merged.merge(reviews, how='inner', left_index=True, right_index=True)\n",
    "    _ = df_fin_merged[df_fin_merged['stars'] >= 3].sort_values('signal', ascending=True).head(5)\n",
    "    _ = _.groupby('app')['signal'].mean().reset_index().sort_values('signal', ascending=True)\n",
    "    answer = translate_name(_['app'].iloc[0])\n",
    "    return(answer)\n",
    "\n",
    "app_name_dict = {'tinder':'Tinder',\n",
    "                 'cmb':'Coffee Meets Bagel',\n",
    "                 'zoosk':'Zoosk',\n",
    "                 'bumble':'Bumble',\n",
    "                 'grindr':'Grindr',\n",
    "                 'hinge':'Hinge',\n",
    "                 'crispa':'Crispa',\n",
    "                 'blk':'Black',\n",
    "                 'match':'Match',\n",
    "                 'east':'East Meets East',\n",
    "                 'jswipe':'Jswipe',\n",
    "                 'league':'The League',\n",
    "                 'farmers':'FarmersOnly',\n",
    "                 'happn':'Happn',\n",
    "                 'okcupid':'Okcupid'}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5001/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Feb/2020 19:45:44] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('term', 'Asian')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2020 19:46:16] \"GET /predict?term=Asian HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee Meets Bagel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2020 19:46:21] \"GET /predict?term=Jewish HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('term', 'Jewish')])\n",
      "Jswipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2020 19:46:25] \"GET /predict?term=Ivy HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('term', 'Ivy')])\n",
      "The League\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2020 19:46:30] \"GET /predict?term=Hookup HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('term', 'Hookup')])\n",
      "Zoosk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Feb/2020 19:46:34] \"GET /predict?term=Catfish HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmutableMultiDict([('term', 'Catfish')])\n",
      "Okcupid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@app.route('/')  # the site to route to, index/main in this case\n",
    "def luanch():\n",
    "    return flask.render_template('predict_app.html')\n",
    "\n",
    "@app.route(\"/predict\"#, methods=[\"POST\",\"GET\"]\n",
    "          )\n",
    "def predict():\n",
    "    print(flask.request.args)\n",
    "    term = flask.request.args\n",
    "    app = get_related_app(term.get('term'))\n",
    "    print(app)\n",
    "    return flask.render_template('predict_app.html', prediction=app)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1',port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
