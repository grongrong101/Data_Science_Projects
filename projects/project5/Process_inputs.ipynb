{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import requests\n",
    "import random\n",
    "import time, os\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from pyzipcode import ZipCodeDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import numpy\n",
    "import shutil\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.layers import Input, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.applications.xception import Xception, preprocess_input, decode_predictions\n",
    "from keras.models import load_model\n",
    "m = load_model('room_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to repickle when classing is done\n",
    "pickle_in = open(\"images_classed_arry/arry_features_exterior.pickle\",\"rb\")\n",
    "arry_features_exterior = pickle.load(pickle_in, encoding='latin1')\n",
    "\n",
    "pickle_in = open(\"images_classed_df/df_features_exterior.pickle\",\"rb\")\n",
    "df_features_exterior = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"images_classed_arry/arry_features_bedroom.pickle\",\"rb\")\n",
    "arry_features_bedroom = pickle.load(pickle_in, encoding='latin1')\n",
    "\n",
    "pickle_in = open(\"images_classed_df/df_features_bedroom.pickle\",\"rb\")\n",
    "df_features_bedroom = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"images_classed_arry/arry_features_bathroom.pickle\",\"rb\")\n",
    "arry_features_bathroom = pickle.load(pickle_in, encoding='latin1')\n",
    "\n",
    "pickle_in = open(\"images_classed_df/df_features_bathroom.pickle\",\"rb\")\n",
    "df_features_bathroom = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"images_classed_arry/arry_features_kitchen.pickle\",\"rb\")\n",
    "arry_features_kitchen = pickle.load(pickle_in, encoding='latin1')\n",
    "\n",
    "pickle_in = open(\"images_classed_df/df_features_kitchen.pickle\",\"rb\")\n",
    "df_features_kitchen = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"images_classed_arry/arry_features_living.pickle\",\"rb\")\n",
    "arry_features_living = pickle.load(pickle_in, encoding='latin1')\n",
    "\n",
    "pickle_in = open(\"images_classed_df/df_features_living.pickle\",\"rb\")\n",
    "df_features_living = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.trulia_db\n",
    "listings = db.listings.find_one({},{'_id':0, 'image_urls':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'input'\n",
    "to_class = {0: 'bathroom',\n",
    " 1: 'bedroom',\n",
    " 2: 'exterior',\n",
    " 3: 'kitchen',\n",
    " 4: 'living',\n",
    " 5: 'plan'}\n",
    "IMG_WIDTH, IMG_HEIGHT = 299, 299 \n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "base_model = Xception(input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "# makes the prediction of the file path image passed as parameter \n",
    "def predict(file, model, to_class):\n",
    "    im = load_img(file, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    x = img_to_array(im)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    index = model.predict(x).argmax()\n",
    "    return to_class[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_input_df(input_fld):\n",
    "    input_dir = os.listdir(input_fld)\n",
    "    img_name_list = []\n",
    "    img_predict_list = []\n",
    "    \n",
    "    for input_img in input_dir:\n",
    "        img_name_list.append(input_img)\n",
    "        img = input_fld+'/'+input_img\n",
    "        img_predict_list.append(predict(img, m, to_class))\n",
    "    comb_img = list(zip(img_name_list,img_predict_list))\n",
    "    df = pd.DataFrame(comb_img, columns = ['img_name', 'img_class']) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appy this to all extract scripts\n",
    "def extract_features(classes, sample_count):\n",
    "    features = np.zeros(shape=(sample_count,10,10, 2048))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=(299, 299),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        classes=[classes])\n",
    "    list_img = [img.split('/') for img in generator.filenames]\n",
    "    img_df = pd.DataFrame(list_img, columns = ['class','img_name'])\n",
    "    img_df['listingid'] = img_df['img_name'].apply(lambda x: x.split('_')[0])\n",
    "    i = 0\n",
    "    #pdb.set_trace()\n",
    "    for inputs_batch in generator:\n",
    "        features_batch = base_model.predict(inputs_batch)\n",
    "        #pdb.set_trace()\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    features = numpy.array(features)\n",
    "    features = np.reshape(features, (sample_count, 10 * 10 * 2048))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moves inputs to right classes\n",
    "def process_input_images(input_img_fld, output_img_fld):\n",
    "    path = input_img_fld\n",
    "    dest_path = output_img_fld\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            source_dir = path+\"/\"+str(img)\n",
    "            room_class = predict(source_dir, m, to_class)\n",
    "            dest_source = dest_path+\"/\"+room_class\n",
    "            shutil.copy(src=source_dir,dst=dest_source)\n",
    "        except:\n",
    "            print('Failed: ' + source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text query\n",
    "def get_close_zipcodes(zip_code,distance):\n",
    "    zcdb = ZipCodeDatabase()\n",
    "    in_radius = [z.zip for z in zcdb.get_zipcodes_around_radius(zip_code, distance)] # ('ZIP', radius in miles)\n",
    "    return in_radius\n",
    "\n",
    "def get_listings(zip_code, radius, min_price=0, max_price=999999999,property_type_in=None):\n",
    "    if property_type_in is None:\n",
    "        property_type = ['apartment','condo','coop','lot/land','mobile/manufactured','multi-family','single-family home','townhouse','unknown']\n",
    "    else:\n",
    "        property_type = [property_type_in]\n",
    "    list_zip_codes = get_close_zipcodes(zip_code, radius)\n",
    "    return db.listings.find({\"$and\":[\n",
    "                                 {\"zip_code\" : {\"$in\" : list_zip_codes}},\n",
    "                                 {\"$expr\": {\"$gte\": [ { \"$toDouble\": \"$price\" }, min_price ]}},\n",
    "                                 {\"$expr\": {\"$lte\": [ { \"$toDouble\": \"$price\" }, max_price ]}},\n",
    "                                 {\"propertytype\": {\"$in\" : property_type}}\n",
    "                                ]}\n",
    "                                 , {'_id':0,'image_urls':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT TEXT QUERY (zipcode, lower price, upper price, type of home ['apartment','condo','coop','lot/land','mobile/manufactured','multi-family','single-family home','townhouse','unknown'] )\n",
    "results_json = get_listings(10314,200, 1.0,500000.0)\n",
    "query_results = pd.DataFrame(results_json)\n",
    "query_result_listingid = query_results['listingid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#step 1, upload pics and break out into groups\n",
    "process_input_images('input/input/upload','input/input/classed_upload')\n",
    "input_df = class_input_df('input/input/upload')\n",
    "base_dir = 'input/input/classed_upload'\n",
    "df_main = pd.DataFrame(data=query_result_listingid)\n",
    "\n",
    "#living\n",
    "living_input_cnt = len(os.listdir(base_dir+\"/living\"))\n",
    "if living_input_cnt >0: \n",
    "    input_living_features = extract_features('living', living_input_cnt)\n",
    "    \n",
    "    dist_living = pairwise_distances(arry_features_living,input_living_features,metric='cosine')\n",
    "    df_dist_living = pd.DataFrame(data=dist_living)\n",
    "    df_dist_living['avg'] = df_dist_living.mean(axis=1)\n",
    "    df_dist_living_sum = df_dist_living.merge(df_features_living, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_living_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_living_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_living = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_living.columns = ['listingid', 'living_avg']\n",
    "    \n",
    "    df_main = df_main.merge(df_fin_living, how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "#kitchen\n",
    "kitchen_input_cnt = len(os.listdir(base_dir+\"/kitchen\"))\n",
    "if kitchen_input_cnt >0: \n",
    "    input_kitchen_features = extract_features('kitchen', kitchen_input_cnt)\n",
    "    \n",
    "    dist_kitchen = pairwise_distances(arry_features_kitchen,input_kitchen_features,metric='cosine')\n",
    "    df_dist_kitchen = pd.DataFrame(data=dist_kitchen)\n",
    "    df_dist_kitchen['avg'] = df_dist_kitchen.mean(axis=1)\n",
    "    df_dist_kitchen_sum = df_dist_kitchen.merge(df_features_kitchen, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_kitchen_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_kitchen_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_kitchen = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_kitchen.columns = ['listingid', 'kitchen_avg']\n",
    "\n",
    "    df_main = df_main.merge(df_fin_kitchen, how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "\n",
    "#exterior\n",
    "exterior_input_cnt = len(os.listdir(base_dir+\"/exterior\"))\n",
    "if exterior_input_cnt >0: \n",
    "    input_exterior_features = extract_features('exterior', exterior_input_cnt)\n",
    "    \n",
    "    dist_exterior = pairwise_distances(arry_features_exterior,input_exterior_features,metric='cosine')\n",
    "    df_dist_exterior = pd.DataFrame(data=dist_exterior)\n",
    "    df_dist_exterior['avg'] = df_dist_exterior.mean(axis=1)\n",
    "    df_dist_exterior_sum = df_dist_exterior.merge(df_features_exterior, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_exterior_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_exterior_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_exterior = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_exterior.columns = ['listingid', 'exterior_avg']\n",
    "\n",
    "    df_main = df_main.merge(df_fin_exterior, how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "\n",
    "#bedroom\n",
    "bedroom_input_cnt = len(os.listdir(base_dir+\"/bedroom\"))\n",
    "if bedroom_input_cnt >0: \n",
    "    input_bedroom_features = extract_features('bedroom', bedroom_input_cnt)\n",
    "    \n",
    "    dist_bedroom = pairwise_distances(arry_features_bedroom,input_bedroom_features,metric='cosine')\n",
    "    df_dist_bedroom = pd.DataFrame(data=dist_bedroom)\n",
    "    df_dist_bedroom['avg'] = df_dist_bedroom.mean(axis=1)\n",
    "    df_dist_bedroom_sum = df_dist_bedroom.merge(df_features_bedroom, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_bedroom_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_bedroom_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_bedroom = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_bedroom.columns = ['listingid', 'bedroom_avg']\n",
    "    \n",
    "    df_main = df_main.merge(df_fin_bedroom,how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "\n",
    "#bathroom\n",
    "bathroom_input_cnt = len(os.listdir(base_dir+\"/bathroom\"))\n",
    "if bathroom_input_cnt >0: \n",
    "    input_bathroom_features = extract_features('bathroom', bathroom_input_cnt)\n",
    "    \n",
    "    dist_bathroom = pairwise_distances(arry_features_bathroom,input_bathroom_features,metric='cosine')\n",
    "    df_dist_bathroom = pd.DataFrame(data=dist_bathroom)\n",
    "    df_dist_bathroom['avg'] = df_dist_bathroom.mean(axis=1)\n",
    "    df_dist_bathroom_sum = df_dist_bathroom.merge(df_features_bathroom, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_bathroom_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_bathroom_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_bathroom = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_bathroom.columns = ['listingid', 'bathroom_avg']\n",
    "    \n",
    "    df_main = df_main.merge(df_fin_bathroom,how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "#plan\n",
    "plan_input_cnt = len(os.listdir(base_dir+\"/plan\"))\n",
    "if plan_input_cnt >0: \n",
    "    input_plan_features = extract_features('plan', plan_input_cnt)\n",
    "    \n",
    "    dist_plan = pairwise_distances(arry_features_plan,input_plan_features,metric='cosine')\n",
    "    df_dist_plan = pd.DataFrame(data=dist_plan)\n",
    "    df_dist_plan['avg'] = df_dist_plan.mean(axis=1)\n",
    "    df_dist_plan_sum = df_dist_plan.merge(df_features_plan, how='inner', left_index=True, right_index=True, left_on=None, right_on=None)\n",
    "    _ = df_dist_plan_sum['listingid'].isin(query_results[\"listingid\"])\n",
    "    df = df_dist_plan_sum[_].groupby([\"listingid\"]).apply(lambda x: x.sort_values([\"avg\"], ascending = False)).reset_index(drop=True)\n",
    "\n",
    "    df2 = df.groupby('listingid').head(2)\n",
    "\n",
    "    df_fin_plan = df2.groupby(['listingid'])['avg'].mean().reset_index()\n",
    "    df_fin_plan.columns = ['listingid', 'plan_avg']\n",
    "    \n",
    "    df_main = df_main.merge(df_fin_plan, how='outer', left_on='listingid', right_on='listingid')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listingid</th>\n",
       "      <th>kitchen_avg</th>\n",
       "      <th>exterior_avg</th>\n",
       "      <th>bedroom_avg</th>\n",
       "      <th>bathroom_avg</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>1032399192</td>\n",
       "      <td>0.506129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.506129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>1041886435</td>\n",
       "      <td>0.516442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>1040039687</td>\n",
       "      <td>0.546675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.546675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>1029809449</td>\n",
       "      <td>0.548864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>5068785559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>1070352788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>1085364671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>5069067839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.638692</td>\n",
       "      <td>0.638692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>5068930206</td>\n",
       "      <td>0.648014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.648014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>1084201444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>5069068068</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.664557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5068798383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5068798383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>5068804688</td>\n",
       "      <td>0.669810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.669810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>5068825517</td>\n",
       "      <td>0.680624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.671933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>5069066150</td>\n",
       "      <td>0.672922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.672922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>5068177037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677597</td>\n",
       "      <td>0.677597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>5068177037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677597</td>\n",
       "      <td>0.677597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>5068770855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6924</th>\n",
       "      <td>5069066154</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>5068108255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693630</td>\n",
       "      <td>0.693630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>5069066399</td>\n",
       "      <td>0.699096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>5068924158</td>\n",
       "      <td>0.701134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>5068922019</td>\n",
       "      <td>0.703827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6684</th>\n",
       "      <td>5069066415</td>\n",
       "      <td>0.713941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.713941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>1034062990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.715125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.715125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>1025537549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>1026903095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>1030320896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7080</th>\n",
       "      <td>5068070096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729938</td>\n",
       "      <td>0.729938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>1041936822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>5069068170</td>\n",
       "      <td>0.734039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.734039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5068834143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>5067962609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.749534</td>\n",
       "      <td>0.749534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>5068699722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.638814</td>\n",
       "      <td>0.869612</td>\n",
       "      <td>0.754213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>5068233602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.755217</td>\n",
       "      <td>0.755217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>1040117535</td>\n",
       "      <td>0.759844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>1026754811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>1084007684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>1084007684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>1034048311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.782664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.782664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>5068658118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783224</td>\n",
       "      <td>0.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>1034179629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>1042527511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>5069066155</td>\n",
       "      <td>0.804108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.804108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>1041937011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.808886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.808886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>1026715260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.809734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.809734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>5069067837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.826828</td>\n",
       "      <td>0.826828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>1041936476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5068903154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       listingid  kitchen_avg  exterior_avg  bedroom_avg  bathroom_avg  \\\n",
       "3344  1032399192     0.506129           NaN          NaN           NaN   \n",
       "4523  1041886435     0.516442           NaN          NaN           NaN   \n",
       "531   1040039687     0.546675           NaN          NaN           NaN   \n",
       "2546  1029809449     0.548864           NaN          NaN           NaN   \n",
       "3458  5068785559          NaN           NaN     0.564916           NaN   \n",
       "4628  1070352788          NaN           NaN     0.625496           NaN   \n",
       "2359  1085364671          NaN           NaN     0.632388           NaN   \n",
       "6690  5069067839          NaN           NaN          NaN      0.638692   \n",
       "2777  5068930206     0.648014           NaN          NaN           NaN   \n",
       "3747  1084201444          NaN           NaN     0.653420           NaN   \n",
       "6801  5069068068     0.664557           NaN          NaN           NaN   \n",
       "165   5068798383          NaN           NaN     0.667505           NaN   \n",
       "164   5068798383          NaN           NaN     0.667505           NaN   \n",
       "5365  5068804688     0.669810           NaN          NaN           NaN   \n",
       "1588  5068825517     0.680624           NaN     0.663243           NaN   \n",
       "6990  5069066150     0.672922           NaN          NaN           NaN   \n",
       "3934  5068177037          NaN           NaN          NaN      0.677597   \n",
       "3935  5068177037          NaN           NaN          NaN      0.677597   \n",
       "263   5068770855          NaN           NaN     0.677744           NaN   \n",
       "6924  5069066154     0.681573           NaN          NaN           NaN   \n",
       "3321  5068108255          NaN           NaN          NaN      0.693630   \n",
       "6890  5069066399     0.699096           NaN          NaN           NaN   \n",
       "5370  5068924158     0.701134           NaN          NaN           NaN   \n",
       "3389  5068922019     0.703827           NaN          NaN           NaN   \n",
       "6684  5069066415     0.713941           NaN          NaN           NaN   \n",
       "3058  1034062990          NaN      0.715125          NaN           NaN   \n",
       "5103  1025537549          NaN      0.726311          NaN           NaN   \n",
       "2999  1026903095          NaN      0.726392          NaN           NaN   \n",
       "1628  1030320896          NaN      0.727458          NaN           NaN   \n",
       "7080  5068070096          NaN           NaN          NaN      0.729938   \n",
       "3566  1041936822          NaN      0.733182          NaN           NaN   \n",
       "6797  5069068170     0.734039           NaN          NaN           NaN   \n",
       "11    5068834143          NaN           NaN     0.745092           NaN   \n",
       "244   5067962609          NaN           NaN          NaN      0.749534   \n",
       "3361  5068699722          NaN           NaN     0.638814      0.869612   \n",
       "5817  5068233602          NaN           NaN          NaN      0.755217   \n",
       "3115  1040117535     0.759844           NaN          NaN           NaN   \n",
       "2603  1026754811          NaN      0.764196          NaN           NaN   \n",
       "2089  1084007684          NaN           NaN     0.776986           NaN   \n",
       "2088  1084007684          NaN           NaN     0.776986           NaN   \n",
       "3248  1034048311          NaN      0.782664          NaN           NaN   \n",
       "1600  5068658118          NaN           NaN          NaN      0.783224   \n",
       "4462  1034179629          NaN      0.792342          NaN           NaN   \n",
       "2915  1042527511          NaN      0.801991          NaN           NaN   \n",
       "6678  5069066155     0.804108           NaN          NaN           NaN   \n",
       "1692  1041937011          NaN      0.808886          NaN           NaN   \n",
       "2702  1026715260          NaN      0.809734          NaN           NaN   \n",
       "6681  5069067837          NaN           NaN          NaN      0.826828   \n",
       "2432  1041936476          NaN      0.908214          NaN           NaN   \n",
       "0     5068903154          NaN           NaN          NaN           NaN   \n",
       "\n",
       "           avg  \n",
       "3344  0.506129  \n",
       "4523  0.516442  \n",
       "531   0.546675  \n",
       "2546  0.548864  \n",
       "3458  0.564916  \n",
       "4628  0.625496  \n",
       "2359  0.632388  \n",
       "6690  0.638692  \n",
       "2777  0.648014  \n",
       "3747  0.653420  \n",
       "6801  0.664557  \n",
       "165   0.667505  \n",
       "164   0.667505  \n",
       "5365  0.669810  \n",
       "1588  0.671933  \n",
       "6990  0.672922  \n",
       "3934  0.677597  \n",
       "3935  0.677597  \n",
       "263   0.677744  \n",
       "6924  0.681573  \n",
       "3321  0.693630  \n",
       "6890  0.699096  \n",
       "5370  0.701134  \n",
       "3389  0.703827  \n",
       "6684  0.713941  \n",
       "3058  0.715125  \n",
       "5103  0.726311  \n",
       "2999  0.726392  \n",
       "1628  0.727458  \n",
       "7080  0.729938  \n",
       "3566  0.733182  \n",
       "6797  0.734039  \n",
       "11    0.745092  \n",
       "244   0.749534  \n",
       "3361  0.754213  \n",
       "5817  0.755217  \n",
       "3115  0.759844  \n",
       "2603  0.764196  \n",
       "2089  0.776986  \n",
       "2088  0.776986  \n",
       "3248  0.782664  \n",
       "1600  0.783224  \n",
       "4462  0.792342  \n",
       "2915  0.801991  \n",
       "6678  0.804108  \n",
       "1692  0.808886  \n",
       "2702  0.809734  \n",
       "6681  0.826828  \n",
       "2432  0.908214  \n",
       "0          NaN  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main['avg'] = df_main.mean(axis=1)\n",
    "df_main.sort_values('avg').head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
